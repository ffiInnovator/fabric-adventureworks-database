{"cells":[{"cell_type":"code","source":["#\n","# Import Packages Required for this Notebook\n","#\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, when, encode, to_date, lit, regexp_replace\n","from pyspark.sql.types import *\n","\n","print(\"Successfully imported all packages for this notebook.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"550bcbb7-5865-469e-9756-5afb1bc2f817"},{"cell_type":"code","source":["#\n","# Configure run-time parameters for this notebook\n","#\n","keep_invalid_records = True\n","\n","layer = \"bronze\"\n","db_schema = \"dbo\"\n","application = \"warehouse\"\n","lakehouse_name = \"AdventureWorks_Lakehouse\"\n","warehouse_name = \"AdventureWorks_Warehouse\"\n","\n","workspace_id = \"3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df\" ## Adv Wrks DE 3 Dev\n","lakehouse_id = \"50402dac-ce50-4831-af2b-7d65ca8fe7db\" ## AdventureWorks_Lakehouse\n","\n","print(\"Successfully configured all paramaters for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"29cf059e-d882-4e3b-9a6d-0a3488137251"},{"cell_type":"code","source":["#\n","# Define the OneLake folder path\n","#\n","folder = \"/Files/\" + layer + \"/\" + application\n","folder_path = \"abfss://\" + workspace_id + \"@onelake.dfs.fabric.microsoft.com/\" + lakehouse_id + folder\n","\n","print(f\"\\nConfigured to process tables from '{lakehouse_name}' into '{warehouse_name}' database schema '{db_schema}' tables.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08472638-bd9d-471f-9e98-c08480b24725"},{"cell_type":"code","source":["#\n","# Create the Spark session\n","#\n","app_name = \"LoadLakehouseToWarehouse\"\n","\n","# Get the current Spark session\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .getOrCreate()\n","\n","print(f\"Spark session {app_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10145ad7-37b2-45f0-be02-3f5f16ce4d14"},{"cell_type":"code","source":["#\n","# Transform & Load the BRONZE layer tables into the SILVER layer\n","#\n","\n","# Regex to remove multi-byte characters\n","# Matches any character outside the standard ASCII range (0-127)\n","pattern = r\"[^\\x00-\\x7F]\"\n","\n","# List all files in the BRONZE layer folder\n","file_list = spark.read.format(\"binaryFile\").load(folder_path).select(\"path\").collect()\n","\n","# Iterate through each file to get the table name, \n","#    then load the corresponding table into a dataframe\n","for file in file_list:\n","    file_path = file[\"path\"]\n","    \n","    if file_path.endswith(\".csv\"):  # Ensure the file is a CSV\n","        # Extract the table name from the file name\n","        table_name = file_path.split(\"/\")[-1].split(\".\")[0]\n","\n","        # Read data from the corresponding Lakehouse table\n","        full_lhse_table_name = lakehouse_name + \".\" + db_schema + \".\" + table_name\n","        lakehouse_df = spark.read.format(\"delta\") \\\n","            .table(full_lhse_table_name)\n","\n","        # Read the existing corresponding warehouse table to get its' schema\n","        full_whse_table_name = warehouse_name + \".\" + db_schema + \".\" + table_name\n","        warehouse_df = spark.read.table(full_whse_table_name)\n","\n","        # Transform lakehouse fields based the warehouse schema's list of fields\n","        transformed_df = lakehouse_df\n","\n","        for field in warehouse_df.schema.fields:\n","            #\n","            # Data Transformation\n","            if isinstance(field.dataType, BinaryType):\n","                transformed_df = transformed_df.withColumn(field.name, encode(col(field.name), \"UTF-8\"))\n","            elif isinstance(field.dataType, BooleanType):\n","                transformed_df = transformed_df.withColumn( field.name, \\\n","                    when(col(field.name) == \"1\", True) \\\n","                    .when(col(field.name) == \"0\", False) \\\n","                    .when(col(field.name) == \"True\", True) \\\n","                    .when(col(field.name) == \"False\", False) \\\n","                    .otherwise(False) \\\n","                )\n","            elif isinstance(field.dataType, StringType):\n","                transformed_df = transformed_df.withColumn(field.name, regexp_replace(col(field.name), pattern, \"\"))\n","            \n","            #\n","            # Data Type Conversion\n","            transformed_df = transformed_df.withColumn(field.name, col(field.name).cast(field.dataType))\n","            \n","            #\n","            # Nullable Transformation\n","            if not field.nullable:\n","                if keep_invalid_records:\n","                    if isinstance(field.dataType, StringType):\n","                        transformed_df = transformed_df.fillna({field.name: \"Unknown\"})\n","                    elif isinstance(field.dataType, DateType)| isinstance(field.dataType, TimestampType):\n","                        transformed_df = transformed_df.fillna({field.name: \"9999-01-01\"})\n","                    elif isinstance(field.dataType, LongType)| isinstance(field.dataType, DecimalType) | \\\n","                        isinstance(field.dataType, DoubleType)| isinstance(field.dataType, FloatType) | \\\n","                        isinstance(field.dataType, IntegerType) | isinstance(field.dataType, ShortType):\n","                        transformed_df = transformed_df.fillna({field.name: 0})\n","                    elif isinstance(field.dataType, BinaryType):\n","                        transformed_df = transformed_df.fillna({field.name: bytearray([0])})\n","                else:\n","                    transformed_df = transformed_df.dropna(subset=[field.name])\n","        ##endfor field\n","\n","        # Write the transformed table to the Warehouse (Lakehouse: See NOTE below.)\n","        # NOTE: Seems to be an issue '403 Forbibben' writing direct to the Warehouse here\n","        #       Workaround is to write to a transient table in the Lakehouse\n","        #       The new CopyJob artifact will then load the conformed tables to the Warehouse\n","        #       An optional next step would the drop all the transient tables in the Lakehouse\n","        transformed_table = lakehouse_name + \".\" + db_schema + \".slv_\" + table_name\n","        spark.sql(f\"DROP TABLE IF EXISTS {transformed_table}\")\n","        transformed_df.write.mode(\"overwrite\") \\\n","            .saveAsTable(transformed_table)\n","        print(f\"Loaded {transformed_df.count():,} transformed data rows into {transformed_table}\")\n","    ##endif .csv file\n","##endfor file"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7b89122c-b949-40eb-86ba-316eb681905d"},{"cell_type":"code","source":["# Stop the Spark session\n","# NOTE: frees up limited F2 SKU capacity resources\n","spark.stop()\n","\n","print(\"Spark session has been stopped successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8d7249ff-bcdf-4d1b-9013-65c383a0e9b6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"50402dac-ce50-4831-af2b-7d65ca8fe7db","default_lakehouse_name":"AdventureWorks_Lakehouse","default_lakehouse_workspace_id":"3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df"},"warehouse":{"known_warehouses":[{"id":"fcbdf876-ae83-4d7b-9fd4-4718c7bad3e2","type":"Datawarehouse"}],"default_warehouse":"fcbdf876-ae83-4d7b-9fd4-4718c7bad3e2"}}},"nbformat":4,"nbformat_minor":5}