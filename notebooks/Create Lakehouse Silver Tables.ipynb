{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08472638-bd9d-471f-9e98-c08480b24725",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-02-05T16:43:19.272923Z",
       "execution_start_time": "2025-02-05T16:43:16.4285635Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "cda6a364-4321-4d81-85cb-8c7e3cf493c1",
       "queued_time": "2025-02-05T16:43:04.2420174Z",
       "session_id": "ab84e929-63f0-4086-b2a8-eba096a3b544",
       "session_start_time": "2025-02-05T16:43:04.2433354Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, ab84e929-63f0-4086-b2a8-eba096a3b544, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramaters for this run:\n",
      "\n",
      "keep_invalid_records = True\n",
      "folder_path = abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse\n",
      "\n",
      "Configured to process tables from 'AdventureWorks_Lakehouse' into 'AdventureWorks_Warehouse' database schema 'dbo' tables.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# CONFIGURE RUN-TIME PARAMETERS FOR THIS NOTEBOOK\n",
    "#\n",
    "keep_invalid_records = True\n",
    "\n",
    "# Define source and target details\n",
    "layer = \"bronze\"\n",
    "db_schema = \"dbo\"\n",
    "application = \"warehouse\"\n",
    "lakehouse_name = \"AdventureWorks_Lakehouse\"\n",
    "warehouse_name = \"AdventureWorks_Warehouse\"\n",
    "\n",
    "\n",
    "# Define the OneLake folder path\n",
    "workspace_id = \" [ YOUR ID HERE ] \" ## Adv Wrks DE 3 Dev\n",
    "lakehouse_id = \" [ YOUR ID HERE ] \" ## AdventureWorks_Lakehouse\n",
    "folder = \"/Files/\" + layer + \"/\" + application\n",
    "folder_path = \"abfss://\" + workspace_id + \"@onelake.dfs.fabric.microsoft.com/\" + lakehouse_id + folder\n",
    "\n",
    "print(\"Paramaters for this run:\")\n",
    "print(f\"\\nkeep_invalid_records = {keep_invalid_records}\")\n",
    "print(f\"folder_path = {folder_path}\")\n",
    "print(f\"\\nConfigured to process tables from '{lakehouse_name}' into '{warehouse_name}' database schema '{db_schema}' tables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10145ad7-37b2-45f0-be02-3f5f16ce4d14",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-02-05T16:43:22.7890398Z",
       "execution_start_time": "2025-02-05T16:43:22.5026241Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "831535bc-64e6-459c-91cd-6f70786a028b",
       "queued_time": "2025-02-05T16:43:22.3789803Z",
       "session_id": "ab84e929-63f0-4086-b2a8-eba096a3b544",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, ab84e929-63f0-4086-b2a8-eba096a3b544, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session LoadLakehouseToWarehouse has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"LoadLakehouseToWarehouse\"\n",
    "\n",
    "# Get the current Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark session {app_name} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b89122c-b949-40eb-86ba-316eb681905d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-02-05T16:45:58.2556031Z",
       "execution_start_time": "2025-02-05T16:43:35.9194326Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "23cce74b-a028-4157-8004-c999c457466b",
       "queued_time": "2025-02-05T16:43:35.7894349Z",
       "session_id": "ab84e929-63f0-4086-b2a8-eba096a3b544",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, ab84e929-63f0-4086-b2a8-eba096a3b544, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 776,286 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactProductInventory\n",
      "Loaded 60,855 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactResellerSales\n",
      "Loaded 60,398 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactInternetSales\n",
      "Loaded 606 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimProduct\n",
      "Loaded 296 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimEmployee\n",
      "Loaded 18,484 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimCustomer\n",
      "Loaded 39,409 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactFinance\n",
      "Loaded 14,264 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactCurrencyRate\n",
      "Loaded 11 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimSalesTerritory\n",
      "Loaded 64,515 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactInternetSalesReason\n",
      "Loaded 2,059 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_ProspectiveBuyer\n",
      "Loaded 3,652 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimDate\n",
      "Loaded 1,911 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactAdditionalInternationalProductDescription\n",
      "Loaded 2,727 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactSurveyResponse\n",
      "Loaded 701 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimReseller\n",
      "Loaded 655 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimGeography\n",
      "Loaded 120 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactCallCenter\n",
      "Loaded 163 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_FactSalesQuota\n",
      "Loaded 99 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimAccount\n",
      "Loaded 16 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimPromotion\n",
      "Loaded 105 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimCurrency\n",
      "Loaded 50 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_NewFactCurrencyRate\n",
      "Loaded 37 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimProductSubcategory\n",
      "Loaded 14 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimOrganization\n",
      "Loaded 10 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimSalesReason\n",
      "Loaded 7 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimDepartmentGroup\n",
      "Loaded 4 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimProductCategory\n",
      "Loaded 3 transformed data rows into AdventureWorks_Lakehouse.dbo.slv_DimScenario\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, encode, to_date, lit, regexp_replace\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Regex to remove multi-byte characters\n",
    "# Matches any character outside the standard ASCII range (0-127)\n",
    "pattern = r\"[^\\x00-\\x7F]\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_list = spark.read.format(\"binaryFile\").load(folder_path).select(\"path\").collect()\n",
    "\n",
    "# Iterate through each file to get the table name, \n",
    "#    then load the corresponding table into a dataframe\n",
    "for file in file_list:\n",
    "    file_path = file[\"path\"]\n",
    "    \n",
    "    if file_path.endswith(\".csv\"):  # Ensure the file is a CSV\n",
    "        # Extract the table name from the file name\n",
    "        table_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        # Read data from the corresponding Lakehouse table\n",
    "        full_lhse_table_name = lakehouse_name + \".\" + db_schema + \".\" + table_name\n",
    "        lakehouse_df = spark.read.format(\"delta\") \\\n",
    "            .table(full_lhse_table_name)\n",
    "\n",
    "        # Read the existing corresponding warehouse table to get its' schema\n",
    "        full_whse_table_name = warehouse_name + \".\" + db_schema + \".\" + table_name\n",
    "        warehouse_df = spark.read.table(full_whse_table_name)\n",
    "\n",
    "        # Transform lakehouse fields based the warehouse schema's list of fields\n",
    "        transformed_df = lakehouse_df\n",
    "\n",
    "        for field in warehouse_df.schema.fields:\n",
    "            #\n",
    "            # Data Transformation\n",
    "            if isinstance(field.dataType, BinaryType):\n",
    "                transformed_df = transformed_df.withColumn(field.name, encode(col(field.name), \"UTF-8\"))\n",
    "            elif isinstance(field.dataType, BooleanType):\n",
    "                transformed_df = transformed_df.withColumn( field.name, \\\n",
    "                    when(col(field.name) == \"1\", True) \\\n",
    "                    .when(col(field.name) == \"0\", False) \\\n",
    "                    .when(col(field.name) == \"True\", True) \\\n",
    "                    .when(col(field.name) == \"False\", False) \\\n",
    "                    .otherwise(False) \\\n",
    "                )\n",
    "            elif isinstance(field.dataType, StringType):\n",
    "                transformed_df = transformed_df.withColumn(field.name, regexp_replace(col(field.name), pattern, \"\"))\n",
    "            \n",
    "            #\n",
    "            # Data Type Conversion\n",
    "            transformed_df = transformed_df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            \n",
    "            #\n",
    "            # Nullable Transformation\n",
    "            if not field.nullable:\n",
    "                if keep_invalid_records:\n",
    "                    if isinstance(field.dataType, StringType):\n",
    "                        transformed_df = transformed_df.fillna({field.name: \"Unknown\"})\n",
    "                    elif isinstance(field.dataType, DateType)| isinstance(field.dataType, TimestampType):\n",
    "                        transformed_df = transformed_df.fillna({field.name: \"9999-01-01\"})\n",
    "                    elif isinstance(field.dataType, LongType)| isinstance(field.dataType, DecimalType) | \\\n",
    "                        isinstance(field.dataType, DoubleType)| isinstance(field.dataType, FloatType) | \\\n",
    "                        isinstance(field.dataType, IntegerType) | isinstance(field.dataType, ShortType):\n",
    "                        transformed_df = transformed_df.fillna({field.name: 0})\n",
    "                    elif isinstance(field.dataType, BinaryType):\n",
    "                        transformed_df = transformed_df.fillna({field.name: bytearray([0])})\n",
    "                else:\n",
    "                    transformed_df = transformed_df.dropna(subset=[field.name])\n",
    "        ##endfor field\n",
    "\n",
    "        # Write the transformed table to the Warehouse (Lakehouse: See NOTE below.)\n",
    "        # NOTE: Seems to be an issue '403 Forbibben' writing direct to the Warehouse here\n",
    "        #       Workaround is to write to a transient table in the Lakehouse\n",
    "        #       The new CopyJob artifact will then load the conformed tables to the Warehouse\n",
    "        #       An optional next step would the drop all the transient tables in the Lakehouse\n",
    "        transformed_table = lakehouse_name + \".\" + db_schema + \".slv_\" + table_name\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {transformed_table}\")\n",
    "        transformed_df.write.mode(\"overwrite\") \\\n",
    "            .saveAsTable(transformed_table)\n",
    "        print(f\"Loaded {transformed_df.count():,} transformed data rows into {transformed_table}\")\n",
    "    ##endif .csv file\n",
    "##endfor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7249ff-bcdf-4d1b-9013-65c383a0e9b6",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"8ef0fd5d-1ddc-4739-a988-5295f11ffa29\",\"activityId\":\"ab84e929-63f0-4086-b2a8-eba096a3b544\",\"applicationId\":\"application_1738773368758_0001\",\"jobGroupId\":\"6\",\"advices\":{\"info\":1}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-02-05T16:49:50.4740972Z",
       "execution_start_time": "2025-02-05T16:49:48.9582177Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "99c3a14e-3b33-4898-98cf-366206ee41e0",
       "queued_time": "2025-02-05T16:49:48.791658Z",
       "session_id": "ab84e929-63f0-4086-b2a8-eba096a3b544",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, ab84e929-63f0-4086-b2a8-eba096a3b544, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session has been stopped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Stop the Spark session\n",
    "# NOTE: frees up limited F2 SKU capacity resources\n",
    "spark.stop()\n",
    "\n",
    "print(\"Spark session has been stopped successfully.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "50402dac-ce50-4831-af2b-7d65ca8fe7db",
    "default_lakehouse_name": "AdventureWorks_Lakehouse",
    "default_lakehouse_workspace_id": "3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df"
   },
   "warehouse": {
    "default_warehouse": "fcbdf876-ae83-4d7b-9fd4-4718c7bad3e2",
    "known_warehouses": [
     {
      "id": "fcbdf876-ae83-4d7b-9fd4-4718c7bad3e2",
      "type": "Datawarehouse"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
