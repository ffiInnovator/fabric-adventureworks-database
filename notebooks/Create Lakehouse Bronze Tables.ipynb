{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7837c5-66b2-41c9-bb1f-94d7846673c1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-02-05T14:58:34.3650414Z",
       "execution_start_time": "2025-02-05T14:58:34.0608092Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f4a5543d-c193-4089-b2c2-b2a5ab4cfbaa",
       "queued_time": "2025-02-05T14:58:33.9060014Z",
       "session_id": "68577344-4bd6-427b-b573-c423d9db2049",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, 68577344-4bd6-427b-b573-c423d9db2049, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured to process files from:\n",
      "abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse\n",
      "into database schema 'dbo' tables.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# CONFIGURE RUN-TIME PARAMETERS FOR THIS NOTEBOOK\n",
    "#\n",
    "\n",
    "layer = \"bronze\"\n",
    "db_schema = \"dbo\"\n",
    "application = \"warehouse\"\n",
    "\n",
    "# Define the OneLake folder path\n",
    "workspace_id = \" [ YOUR ID HERE ] \" ## Adv Wrks DE 3 Dev\n",
    "lakehouse_id = \" [ YOUR ID HERE ] \" ## AdventureWorks_Lakehouse\n",
    "folder = \"/Files/\" + layer + \"/\" + application\n",
    "folder_path = \"abfss://\" + workspace_id + \"@onelake.dfs.fabric.microsoft.com/\" + lakehouse_id + folder\n",
    "\n",
    "print(f\"Configured to process files from:\\n{folder_path}\\ninto database schema '{db_schema}' tables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ebabe6-a668-4dc2-b0de-333625bd1ca1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-02-05T14:52:57.9684181Z",
       "execution_start_time": "2025-02-05T14:52:55.1117052Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "5733ca20-4368-4ab7-888a-064422601f25",
       "queued_time": "2025-02-05T14:52:36.7631332Z",
       "session_id": "68577344-4bd6-427b-b573-c423d9db2049",
       "session_start_time": "2025-02-05T14:52:36.7644376Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 68577344-4bd6-427b-b573-c423d9db2049, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session LoadLakehouseTables has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"LoadLakehouseBronzeTables\"\n",
    "\n",
    "# Get the current Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark session {app_name} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585b8031-0c8e-4281-bc0b-1c7cf5eebcc7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-02-05T14:58:58.6483836Z",
       "execution_start_time": "2025-02-05T14:58:58.4118771Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c0c598cf-434a-446c-ade5-69b48a52b56a",
       "queued_time": "2025-02-05T14:58:58.2761275Z",
       "session_id": "68577344-4bd6-427b-b573-c423d9db2049",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 68577344-4bd6-427b-b573-c423d9db2049, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function 'tsql_to_structype' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from typing import List\n",
    "\n",
    "def tsql_to_structype(SqlScript: List) -> StructType:\n",
    "    \"\"\"\n",
    "    Converts a T-SQL CREATE TABLE script into a PySpark StructType schema.\n",
    "\n",
    "    Parameters:\n",
    "        SqlScript List[str]: The T-SQL script defining the table structure.\n",
    "\n",
    "    Returns:\n",
    "        StructType: A PySpark StructType object representing the schema.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mapping from T-SQL data types to PySpark data types\n",
    "    # NOTE: The Lakehouse is the implementation of the BRONZE layer\n",
    "    #       It is the target for all RAW data\n",
    "    #       String is the only data type used, ensureing all rows are read\n",
    "    #       Data formats and contraints will be applied next in the SILVER layer\n",
    "    sql_to_spark_type = {\n",
    "        \"bigint\": StringType(),\n",
    "        \"binary\": StringType(),\n",
    "\t\t\"bit\": StringType(),\n",
    "        \"char\": StringType(),\n",
    "        \"date\": StringType(),\n",
    "        \"datetime\": StringType(),\n",
    "        \"datetime2\": StringType(),\n",
    "        \"datetimeoffset\": StringType(),\n",
    "        \"decimal\": StringType(),\n",
    "        \"double\": StringType(),\n",
    "        \"float\": StringType(),\n",
    "        \"image\": StringType(),\n",
    "        \"int\": StringType(),\n",
    "        \"money\": StringType(),\n",
    "        \"nchar\": StringType(),\n",
    "        \"nvarchar\": StringType(),\n",
    "        \"smallint\": StringType(),\n",
    "        \"text\": StringType(),\n",
    "        \"time\": StringType(),\n",
    "        \"tinyint\": StringType(),\n",
    "        \"uniqueidentifier\": StringType(),\n",
    "        \"varbinary\": StringType(),\n",
    "        \"varchar\": StringType() \n",
    "    }\n",
    "    \n",
    "    # Extract column definitions from the SQL script\n",
    "    fields = []\n",
    "    for line in SqlScript:\n",
    "        line = line.strip()\n",
    "        # Skip irrelevant lines\n",
    "        if line.startswith(\"[\") and \"]\" in line and \"[\" in line:\n",
    "            column_name = line.split(\"[\")[1].split(\"]\")[0]\n",
    "            column_type = line.split(\"[\")[2].split(\"]\")[0].lower()\n",
    "            nullable = \"NOT NULL\" not in line\n",
    "            \n",
    "            # Get the PySpark type or default to StringType\n",
    "            spark_type = sql_to_spark_type.get(column_type.split(\"(\")[0], StringType())\n",
    "            fields.append(StructField(column_name, spark_type, nullable))\n",
    "    \n",
    "    return StructType(fields)\n",
    "\n",
    "print(\"The function 'tsql_to_structype' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d8f993-1ac5-486f-b023-2ffb1fd44653",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-02-05T15:01:40.903704Z",
       "execution_start_time": "2025-02-05T15:00:33.6756082Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1726e45c-83d5-4602-85e1-1d468ab7d26c",
       "queued_time": "2025-02-05T15:00:33.501408Z",
       "session_id": "68577344-4bd6-427b-b573-c423d9db2049",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 68577344-4bd6-427b-b573-c423d9db2049, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactProductInventory.csv into table dbo.FactProductInventory\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactResellerSales.csv into table dbo.FactResellerSales\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactInternetSales.csv into table dbo.FactInternetSales\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimProduct.csv into table dbo.DimProduct\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimEmployee.csv into table dbo.DimEmployee\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimCustomer.csv into table dbo.DimCustomer\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactFinance.csv into table dbo.FactFinance\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactCurrencyRate.csv into table dbo.FactCurrencyRate\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimSalesTerritory.csv into table dbo.DimSalesTerritory\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactInternetSalesReason.csv into table dbo.FactInternetSalesReason\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/ProspectiveBuyer.csv into table dbo.ProspectiveBuyer\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimDate.csv into table dbo.DimDate\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactAdditionalInternationalProductDescription.csv into table dbo.FactAdditionalInternationalProductDescription\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactSurveyResponse.csv into table dbo.FactSurveyResponse\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimReseller.csv into table dbo.DimReseller\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimGeography.csv into table dbo.DimGeography\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactCallCenter.csv into table dbo.FactCallCenter\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/FactSalesQuota.csv into table dbo.FactSalesQuota\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimAccount.csv into table dbo.DimAccount\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimPromotion.csv into table dbo.DimPromotion\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimCurrency.csv into table dbo.DimCurrency\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/NewFactCurrencyRate.csv into table dbo.NewFactCurrencyRate\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimProductSubcategory.csv into table dbo.DimProductSubcategory\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimOrganization.csv into table dbo.DimOrganization\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimSalesReason.csv into table dbo.DimSalesReason\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimDepartmentGroup.csv into table dbo.DimDepartmentGroup\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimProductCategory.csv into table dbo.DimProductCategory\n",
      "Loaded file abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse/DimScenario.csv into table dbo.DimScenario\n"
     ]
    }
   ],
   "source": [
    "# List all files in the folder\n",
    "file_list = spark.read.format(\"binaryFile\").load(folder_path).select(\"path\").collect()\n",
    "\n",
    "# Iterate through each file and load it into a table\n",
    "for file in file_list:\n",
    "    file_path = file[\"path\"]\n",
    "    \n",
    "    if file_path.endswith(\".csv\"):  # Ensure the file is a CSV\n",
    "        # Extract the table name from the file name\n",
    "        table_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        full_table_name = db_schema + \".\" + table_name\n",
    "\n",
    "        # Read the sql script file for this table into a dataframe\n",
    "        script_file_path = folder_path + \"/\" + table_name + \".sql\"\n",
    "        df_script = spark.read.text(script_file_path)\n",
    "        \n",
    "        # Convert the one-column DataFrame to a string array using collect()\n",
    "        string_array = [row[\"value\"] for row in df_script.collect()]\n",
    "\n",
    "        # Convert the sql script into a pyspark schema structure\n",
    "        schema = tsql_to_structype(string_array)\n",
    "\n",
    "        # Read the csv file into source df\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"false\") \\\n",
    "            .option(\"delimiter\", \"|\") \\\n",
    "            .schema(schema) \\\n",
    "            .load(file_path)\n",
    "        \n",
    "        # Save the DataFrame as a table\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "        df.write.mode(\"overwrite\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "        \n",
    "        print(f\"Loaded file {file_path} into table {full_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39bb798c-2521-4b52-88b0-4597bf9e6e8d",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"40a1e640-d55a-4b18-8174-d004d74415b6\",\"activityId\":\"68577344-4bd6-427b-b573-c423d9db2049\",\"applicationId\":\"application_1738766626591_0001\",\"jobGroupId\":\"10\",\"advices\":{\"info\":1}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-02-05T15:01:54.5763681Z",
       "execution_start_time": "2025-02-05T15:01:53.0836891Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7313b0b4-c380-4c1c-9ca1-a53d4b2f6716",
       "queued_time": "2025-02-05T15:01:52.9533067Z",
       "session_id": "68577344-4bd6-427b-b573-c423d9db2049",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 68577344-4bd6-427b-b573-c423d9db2049, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session has been stopped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Stop the Spark session\n",
    "# NOTE: frees up limited F2 SKU capacity resources\n",
    "spark.stop()\n",
    "\n",
    "print(\"Spark session has been stopped successfully.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "50402dac-ce50-4831-af2b-7d65ca8fe7db",
    "default_lakehouse_name": "AdventureWorks_Lakehouse",
    "default_lakehouse_workspace_id": "3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df",
    "known_lakehouses": [
     {
      "id": "50402dac-ce50-4831-af2b-7d65ca8fe7db"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
