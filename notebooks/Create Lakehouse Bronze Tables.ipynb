{"cells":[{"cell_type":"code","source":["#\n","# Import Packages Required for this Notebook\n","#\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","from typing import List\n","\n","print(\"Successfully imported all packages for this notebook.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26828463-eea1-46d6-bcfc-b0b73f1c8c6a"},{"cell_type":"code","source":["#\n","# Configure run-time parameters for this notebook\n","#\n","layer = \"bronze\"\n","db_schema = \"dbo\"\n","application = \"warehouse\"\n","workspace_id = \"3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df\" ## Adv Wrks DE 3 Dev\n","lakehouse_id = \"50402dac-ce50-4831-af2b-7d65ca8fe7db\" ## AdventureWorks_Lakehouse\n","\n","print(\"Successfully configured all paramaters for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fc7837c5-66b2-41c9-bb1f-94d7846673c1"},{"cell_type":"code","source":["#\n","# Define the OneLake folder path\n","#\n","folder = \"/Files/\" + layer + \"/\" + application\n","folder_path = \"abfss://\" + workspace_id + \"@onelake.dfs.fabric.microsoft.com/\" + lakehouse_id + folder\n","\n","print(f\"Configured to process files from:\\n{folder_path}\\ninto database schema '{db_schema}' tables.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c32c8df9-e11d-4516-bb89-475c95959811"},{"cell_type":"code","source":["#\n","# Create the Spark session\n","#\n","app_name = \"LoadLakehouseBronzeTables\"\n","\n","# Get the current Spark session\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .getOrCreate()\n","\n","print(f\"Spark session {app_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e7ebabe6-a668-4dc2-b0de-333625bd1ca1"},{"cell_type":"code","source":["#\n","# Define the fuction to create a PySpark Stuct fro a T-SQL script\n","#\n","def tsql_to_structype(SqlScript: List) -> StructType:\n","    \"\"\"\n","    Converts a T-SQL CREATE TABLE script into a PySpark StructType schema.\n","\n","    Parameters:\n","        SqlScript List[str]: The T-SQL script defining the table structure.\n","\n","    Returns:\n","        StructType: A PySpark StructType object representing the schema.\n","    \"\"\"\n","    \n","    # Mapping from T-SQL data types to PySpark data types\n","    # NOTE: The Lakehouse is the implementation of the BRONZE layer\n","    #       It is the target for all RAW data\n","    #       String is the only data type used, ensureing all rows are read\n","    #       Data formats and contraints will be applied next in the SILVER layer\n","    sql_to_spark_type = {\n","        \"bigint\": StringType(),\n","        \"binary\": StringType(),\n","\t\t\"bit\": StringType(),\n","        \"char\": StringType(),\n","        \"date\": StringType(),\n","        \"datetime\": StringType(),\n","        \"datetime2\": StringType(),\n","        \"datetimeoffset\": StringType(),\n","        \"decimal\": StringType(),\n","        \"double\": StringType(),\n","        \"float\": StringType(),\n","        \"image\": StringType(),\n","        \"int\": StringType(),\n","        \"money\": StringType(),\n","        \"nchar\": StringType(),\n","        \"nvarchar\": StringType(),\n","        \"smallint\": StringType(),\n","        \"text\": StringType(),\n","        \"time\": StringType(),\n","        \"tinyint\": StringType(),\n","        \"uniqueidentifier\": StringType(),\n","        \"varbinary\": StringType(),\n","        \"varchar\": StringType() \n","    }\n","    \n","    # Extract column definitions from the SQL script\n","    fields = []\n","    for line in SqlScript:\n","        line = line.strip()\n","        # Skip irrelevant lines\n","        if line.startswith(\"[\") and \"]\" in line and \"[\" in line:\n","            column_name = line.split(\"[\")[1].split(\"]\")[0]\n","            column_type = line.split(\"[\")[2].split(\"]\")[0].lower()\n","            nullable = \"NOT NULL\" not in line\n","            \n","            # Get the PySpark type or default to StringType\n","            spark_type = sql_to_spark_type.get(column_type.split(\"(\")[0], StringType())\n","            fields.append(StructField(column_name, spark_type, nullable))\n","    \n","    return StructType(fields)\n","\n","print(\"The function 'tsql_to_structype' has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"585b8031-0c8e-4281-bc0b-1c7cf5eebcc7"},{"cell_type":"code","source":["#\n","# Load the BRONZE layer files into a Lakehouse table\n","#\n","file_list = spark.read.format(\"binaryFile\").load(folder_path).select(\"path\").collect()\n","\n","# Iterate through each file and load it into a table\n","for file in file_list:\n","    file_path = file[\"path\"]\n","    \n","    if file_path.endswith(\".csv\"):  # Ensure the file is a CSV\n","        # Extract the table name from the file name\n","        table_name = file_path.split(\"/\")[-1].split(\".\")[0]\n","        full_table_name = db_schema + \".\" + table_name\n","\n","        # Read the sql script file for this table into a dataframe\n","        script_file_path = folder_path + \"/\" + table_name + \".sql\"\n","        df_script = spark.read.text(script_file_path)\n","        \n","        # Convert the one-column DataFrame to a string array using collect()\n","        string_array = [row[\"value\"] for row in df_script.collect()]\n","\n","        # Convert the sql script into a pyspark schema structure\n","        schema = tsql_to_structype(string_array)\n","\n","        # Read the csv file into source df\n","        df = spark.read.format(\"csv\") \\\n","            .option(\"header\", \"false\") \\\n","            .option(\"delimiter\", \"|\") \\\n","            .schema(schema) \\\n","            .load(file_path)\n","        \n","        # Save the DataFrame as a table\n","        spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n","        df.write.mode(\"overwrite\") \\\n","            .option(\"mergeSchema\", \"true\") \\\n","            .saveAsTable(full_table_name)\n","        \n","        print(f\"Loaded file {file_path} into table {full_table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"90d8f993-1ac5-486f-b023-2ffb1fd44653"},{"cell_type":"code","source":["#\n","# Stop the Spark session\n","# NOTE: frees up limited F2 SKU capacity resources\n","#\n","spark.stop()\n","\n","print(\"Spark session has been stopped successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"39bb798c-2521-4b52-88b0-4597bf9e6e8d"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"50402dac-ce50-4831-af2b-7d65ca8fe7db","default_lakehouse_name":"AdventureWorks_Lakehouse","default_lakehouse_workspace_id":"3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df","known_lakehouses":[{"id":"50402dac-ce50-4831-af2b-7d65ca8fe7db"}]}}},"nbformat":4,"nbformat_minor":5}