{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89742b5c-8a0c-4f2b-aa5b-c792d5a597c2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-01-31T16:27:45.1348786Z",
       "execution_start_time": "2025-01-31T16:27:44.8944031Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "810871fb-5c4d-4ae2-aa9a-950b86c030ea",
       "queued_time": "2025-01-31T16:27:44.7291781Z",
       "session_id": "c2df6915-7f77-4648-a831-0e623f5b6ec9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, c2df6915-7f77-4648-a831-0e623f5b6ec9, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramaters for this run:\n",
      "transient_tables = True\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Notebook Paramaters\n",
    "##\n",
    "transient_tables = True\n",
    "\n",
    "print(\"Paramaters for this run:\")\n",
    "print(f\"transient_tables = {transient_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e87136-aacb-44bd-b8ab-9f9e5193a9ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-01-31T16:27:48.1856781Z",
       "execution_start_time": "2025-01-31T16:27:47.9602698Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "05390e39-1769-44d9-a833-364d93b5d160",
       "queued_time": "2025-01-31T16:27:47.7873167Z",
       "session_id": "c2df6915-7f77-4648-a831-0e623f5b6ec9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, c2df6915-7f77-4648-a831-0e623f5b6ec9, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session DropLakehouseTables has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"DropLakehouseTables\"\n",
    "\n",
    "# Get the current Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark session {app_name} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f5dfb05-c2a4-4695-9b71-a74e17d5b316",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-01-31T16:27:51.7403267Z",
       "execution_start_time": "2025-01-31T16:27:51.4747416Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f3f4c2ce-edf9-4b9c-a5da-e66ba2d85a3b",
       "queued_time": "2025-01-31T16:27:51.3308856Z",
       "session_id": "c2df6915-7f77-4648-a831-0e623f5b6ec9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, c2df6915-7f77-4648-a831-0e623f5b6ec9, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary 'layer_to_schema' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "layer_to_schema = {\n",
    "    \"default\": \"dbo\",\n",
    "    \"bronze\": \"brz\",\n",
    "    \"silver\": \"slv\",\n",
    "    \"gold\": \"gld\"\n",
    "}\n",
    "\n",
    "print(\"The dictionary 'layer_to_schema' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa15424-5a4a-46e8-b1ae-857d89d7f19d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-01-31T16:28:05.3733074Z",
       "execution_start_time": "2025-01-31T16:28:05.1228352Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "fe4642e4-109a-4e16-bcfa-9519d94e8d8d",
       "queued_time": "2025-01-31T16:28:04.9599508Z",
       "session_id": "c2df6915-7f77-4648-a831-0e623f5b6ec9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, c2df6915-7f77-4648-a831-0e623f5b6ec9, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured to process from 'bronze' layer files in:\n",
      "abfss://3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df@onelake.dfs.fabric.microsoft.com/50402dac-ce50-4831-af2b-7d65ca8fe7db/Files/bronze/warehouse\n"
     ]
    }
   ],
   "source": [
    "# Configure layer and schema where tables are to be deleted\n",
    "layer = \"bronze\"\n",
    "db_schema = layer_to_schema.get(layer, \"NOT_FOUND\")\n",
    "application = \"warehouse\"\n",
    "\n",
    "# Define the OneLake folder path\n",
    "workspace_id = \" [ YOUR ID HERE ] \" ## Adv Wrks DE 3 Dev\n",
    "lakehouse_id = \" [ YOUR ID HERE ] \" ## AdventureWorks_Lakehouse\n",
    "folder = \"/Files/\" + layer + \"/\" + application\n",
    "folder_path = \"abfss://\" + workspace_id + \"@onelake.dfs.fabric.microsoft.com/\" + lakehouse_id + folder\n",
    "\n",
    "print(f\"Configured to process from '{layer}' layer files in:\\n{folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85567502-fd33-4eff-9067-9061f3b8e094",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-01-31T16:28:49.4503574Z",
       "execution_start_time": "2025-01-31T16:28:48.6750003Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "fb2fe4c6-306a-4d2a-8621-75023845f876",
       "queued_time": "2025-01-31T16:28:48.5220901Z",
       "session_id": "c2df6915-7f77-4648-a831-0e623f5b6ec9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, c2df6915-7f77-4648-a831-0e623f5b6ec9, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped table brz.slv_DimAccount\n",
      "Dropped table brz.slv_DimCurrency\n",
      "Dropped table brz.slv_DimCustomer\n",
      "Dropped table brz.slv_DimDate\n",
      "Dropped table brz.slv_DimDepartmentGroup\n",
      "Dropped table brz.slv_DimEmployee\n",
      "Dropped table brz.slv_DimGeography\n",
      "Dropped table brz.slv_DimOrganization\n",
      "Dropped table brz.slv_DimProduct\n",
      "Dropped table brz.slv_DimProductCategory\n",
      "Dropped table brz.slv_DimProductSubcategory\n",
      "Dropped table brz.slv_DimPromotion\n",
      "Dropped table brz.slv_DimReseller\n",
      "Dropped table brz.slv_DimSalesReason\n",
      "Dropped table brz.slv_DimSalesTerritory\n",
      "Dropped table brz.slv_DimScenario\n",
      "Dropped table brz.slv_FactAdditionalInternationalProductDescription\n",
      "Dropped table brz.slv_FactCallCenter\n",
      "Dropped table brz.slv_FactCurrencyRate\n",
      "Dropped table brz.slv_FactFinance\n",
      "Dropped table brz.slv_FactInternetSales\n",
      "Dropped table brz.slv_FactInternetSalesReason\n",
      "Dropped table brz.slv_FactProductInventory\n",
      "Dropped table brz.slv_FactResellerSales\n",
      "Dropped table brz.slv_FactSalesQuota\n",
      "Dropped table brz.slv_FactSurveyResponse\n",
      "Dropped table brz.slv_NewFactCurrencyRate\n",
      "Dropped table brz.slv_ProspectiveBuyer\n"
     ]
    }
   ],
   "source": [
    "# Get all files in the folder\n",
    "file_list = spark.read.format(\"binaryFile\").load(folder_path).select(\"path\").collect()\n",
    "\n",
    "# Drop all tables to to associated CSV data\n",
    "for file in sorted(file_list):\n",
    "    file_path = file[\"path\"]\n",
    "    if file_path.endswith(\".csv\"):\n",
    "        # Extract the table name from the file name\n",
    "        table_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        \n",
    "        # Drop the table\n",
    "        if transient_tables:\n",
    "            full_table_name = db_schema + \".\" + layer_to_schema.get(\"silver\", \"NOT_FOUND\") +\"_\" + table_name\n",
    "        else:\n",
    "            full_table_name = db_schema + \".\" + table_name\n",
    "        \n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "        print(f\"Dropped table {full_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe05374-c1bb-41a9-a269-9171e69e206d",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"3fa82e3a-623e-4848-82d2-2ec3260b3fa4\",\"activityId\":\"c2df6915-7f77-4648-a831-0e623f5b6ec9\",\"applicationId\":\"application_1738340512800_0001\",\"jobGroupId\":\"10\",\"advices\":{\"info\":1}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-01-31T16:29:11.8757226Z",
       "execution_start_time": "2025-01-31T16:29:10.3756796Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "0bc0de44-31c7-403e-a6b2-da78d7818b38",
       "queued_time": "2025-01-31T16:29:10.2392766Z",
       "session_id": "c2df6915-7f77-4648-a831-0e623f5b6ec9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, c2df6915-7f77-4648-a831-0e623f5b6ec9, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session has been stopped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Stop the Spark session\n",
    "# NOTE: frees up limited F2 SKU capacity resources\n",
    "spark.stop()\n",
    "\n",
    "print(\"Spark session has been stopped successfully.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "50402dac-ce50-4831-af2b-7d65ca8fe7db",
    "default_lakehouse_name": "AdventureWorks_Lakehouse",
    "default_lakehouse_workspace_id": "3ac7ce42-ae74-4e7d-8ac3-5ce8358a30df"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
